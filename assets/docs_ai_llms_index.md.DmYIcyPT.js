import{_ as t,c as d,o as r,a4 as a}from"./chunks/framework.I1cCAST3.js";const e="/assets/llms.D2PJxED4.png",m=JSON.parse('{"title":"模型列表","description":"","frontmatter":{},"headers":[],"relativePath":"docs/ai/llms/index.md","filePath":"docs/ai/llms/index.md"}'),o={name:"docs/ai/llms/index.md"},n=a('<h1 id="模型列表" tabindex="-1">模型列表 <a class="header-anchor" href="#模型列表" aria-label="Permalink to &quot;模型列表&quot;">​</a></h1><p>Chaterm 支持多种模型提供商，提供灵活的 AI 编程体验。从内置模型到自定义集成，满足不同场景的需求。</p><h2 id="内置模型" tabindex="-1">内置模型 <a class="header-anchor" href="#内置模型" aria-label="Permalink to &quot;内置模型&quot;">​</a></h2><p><img src="'+e+'" alt=""></p><p>Chaterm 内置了多个优质的代码模型，开箱即用，无需额外配置：</p><h3 id="思维链模型" tabindex="-1">思维链模型 <a class="header-anchor" href="#思维链模型" aria-label="Permalink to &quot;思维链模型&quot;">​</a></h3><p>这些模型具备深度推理能力，能够逐步分析问题并给出详细的解决方案：</p><table tabindex="0"><thead><tr><th>模型</th><th>特点</th><th>适用场景</th><th>推理能力</th></tr></thead><tbody><tr><td><strong>DeepSeek-R1 (thinking)</strong></td><td>具备深度推理能力的先进模型</td><td>复杂算法设计、架构分析</td><td>高</td></tr><tr><td><strong>GLM-4.6 (thinking)</strong></td><td>强大的逻辑推理能力</td><td>代码审查、问题诊断</td><td>中高</td></tr><tr><td><strong>Qwen-Plus (thinking)</strong></td><td>阿里云通义千问思维链模型</td><td>多语言开发、跨平台项目</td><td>中高</td></tr></tbody></table><h3 id="标准模型" tabindex="-1">标准模型 <a class="header-anchor" href="#标准模型" aria-label="Permalink to &quot;标准模型&quot;">​</a></h3><p>快速响应的标准模型，适合日常编程任务：</p><table tabindex="0"><thead><tr><th>模型</th><th>特点</th><th>适用场景</th><th>响应速度</th></tr></thead><tbody><tr><td><strong>DeepSeek-V3.2</strong></td><td>支持复杂代码分析</td><td>大型项目重构、性能优化</td><td>快</td></tr><tr><td><strong>Qwen-Plus</strong></td><td>高性能代码生成模型</td><td>企业级应用开发</td><td>快</td></tr><tr><td><strong>GLM-4.6</strong></td><td>优秀的代码生成能力</td><td>快速原型开发、功能实现</td><td>中</td></tr><tr><td><strong>Qwen-Turbo</strong></td><td>快速响应的轻量级模型</td><td>实时编程辅助、快速迭代</td><td>很快</td></tr></tbody></table><h2 id="添加自定义模型" tabindex="-1">添加自定义模型 <a class="header-anchor" href="#添加自定义模型" aria-label="Permalink to &quot;添加自定义模型&quot;">​</a></h2><p>您可以在设置中添加更多模型提供商，扩展 Chaterm 的功能。支持多种集成方式，满足不同需求：</p><h3 id="模型集成" tabindex="-1">模型集成 <a class="header-anchor" href="#模型集成" aria-label="Permalink to &quot;模型集成&quot;">​</a></h3><h4 id="_1-litellm-集成" tabindex="-1">1. LiteLLM 集成 <a class="header-anchor" href="#_1-litellm-集成" aria-label="Permalink to &quot;1. LiteLLM 集成&quot;">​</a></h4><p>通过 LiteLLM 可以连接多种模型提供商，支持统一的 API 接口：</p><table tabindex="0"><thead><tr><th>配置项</th><th>说明</th><th>是否必需</th></tr></thead><tbody><tr><td><strong>URL 地址</strong></td><td>LiteLLM 服务端点</td><td>必需</td></tr><tr><td><strong>API Key</strong></td><td>访问密钥</td><td>必需</td></tr><tr><td><strong>模型名称</strong></td><td>要使用的具体模型</td><td>必需</td></tr></tbody></table><p><strong>优势：</strong> 统一接口，支持多种模型提供商</p><h4 id="_2-openai-集成" tabindex="-1">2. OpenAI 集成 <a class="header-anchor" href="#_2-openai-集成" aria-label="Permalink to &quot;2. OpenAI 集成&quot;">​</a></h4><p>直接连接 OpenAI 服务，使用官方 API：</p><table tabindex="0"><thead><tr><th>配置项</th><th>说明</th><th>是否必需</th></tr></thead><tbody><tr><td><strong>OpenAI URL 地址</strong></td><td>OpenAI API 端点</td><td>必需</td></tr><tr><td><strong>OpenAI API Key</strong></td><td>OpenAI 访问密钥</td><td>必需</td></tr><tr><td><strong>模型名称</strong></td><td>GPT-5、GPT-4 等</td><td>必需</td></tr></tbody></table><p><strong>优势：</strong> 官方支持，稳定可靠</p><h4 id="_3-amazon-bedrock" tabindex="-1">3. Amazon Bedrock <a class="header-anchor" href="#_3-amazon-bedrock" aria-label="Permalink to &quot;3. Amazon Bedrock&quot;">​</a></h4><p>使用 AWS Bedrock 服务，企业级解决方案：</p><table tabindex="0"><thead><tr><th>配置项</th><th>说明</th><th>是否必需</th></tr></thead><tbody><tr><td><strong>AWS Access Key</strong></td><td>AWS 访问密钥</td><td>必需</td></tr><tr><td><strong>AWS Secret Key</strong></td><td>AWS 秘密密钥</td><td>必需</td></tr><tr><td><strong>AWS Session Token</strong></td><td>会话令牌</td><td>可选</td></tr><tr><td><strong>AWS 区域</strong></td><td>服务区域</td><td>必需</td></tr><tr><td><strong>自定义 VPC 端点</strong></td><td>私有网络端点</td><td>可选</td></tr><tr><td><strong>跨区域推理</strong></td><td>多区域部署</td><td>可选</td></tr><tr><td><strong>模型名称</strong></td><td>Bedrock 模型</td><td>必需</td></tr></tbody></table><p><strong>优势：</strong> 企业级安全，高可用性</p><h4 id="_4-deepseek-集成" tabindex="-1">4. DeepSeek 集成 <a class="header-anchor" href="#_4-deepseek-集成" aria-label="Permalink to &quot;4. DeepSeek 集成&quot;">​</a></h4><p>连接 DeepSeek 官方 API，使用先进模型：</p><table tabindex="0"><thead><tr><th>配置项</th><th>说明</th><th>是否必需</th></tr></thead><tbody><tr><td><strong>DeepSeek API Key</strong></td><td>DeepSeek 访问密钥</td><td>必需</td></tr><tr><td><strong>模型名称</strong></td><td>DeepSeek 模型</td><td>必需</td></tr></tbody></table><p><strong>优势：</strong> 先进模型，推理能力强</p><h3 id="本地模型部署" tabindex="-1">本地模型部署 <a class="header-anchor" href="#本地模型部署" aria-label="Permalink to &quot;本地模型部署&quot;">​</a></h3><h4 id="_5-ollama-本地部署" tabindex="-1">5. Ollama 本地部署 <a class="header-anchor" href="#_5-ollama-本地部署" aria-label="Permalink to &quot;5. Ollama 本地部署&quot;">​</a></h4><p>使用本地部署的 Ollama 模型，保护数据隐私：</p><table tabindex="0"><thead><tr><th>配置项</th><th>说明</th><th>是否必需</th></tr></thead><tbody><tr><td><strong>Ollama URL 地址</strong></td><td>本地 Ollama 服务地址</td><td>必需</td></tr><tr><td><strong>模型名称</strong></td><td>本地模型名称</td><td>必需</td></tr></tbody></table><p><strong>优势：</strong> 数据隐私，离线可用</p><h2 id="使用说明" tabindex="-1">使用说明 <a class="header-anchor" href="#使用说明" aria-label="Permalink to &quot;使用说明&quot;">​</a></h2><h3 id="快速开始" tabindex="-1">快速开始 <a class="header-anchor" href="#快速开始" aria-label="Permalink to &quot;快速开始&quot;">​</a></h3><ol><li><strong>进入设置页面</strong> - 点击右上角设置图标</li><li><strong>选择&quot;模型&quot;选项卡</strong> - 在左侧菜单中找到模型设置</li><li><strong>点击&quot;添加模型&quot;按钮</strong> - 开始添加新的模型配置</li><li><strong>选择相应的提供商</strong> - 根据需求选择合适的模型提供商</li><li><strong>填写必要的配置信息</strong> - 按照表格要求填写配置项</li><li><strong>保存并测试连接</strong> - 验证配置是否正确</li></ol><h3 id="配置技巧" tabindex="-1">配置技巧 <a class="header-anchor" href="#配置技巧" aria-label="Permalink to &quot;配置技巧&quot;">​</a></h3><ul><li><strong>API Key 安全</strong>：使用环境变量存储敏感信息</li><li><strong>连接测试</strong>：配置完成后务必进行连接测试</li><li><strong>模型切换</strong>：可以配置多个模型，根据需要切换使用</li><li><strong>性能监控</strong>：关注模型响应时间和使用成本</li></ul><h2 id="模型选择建议" tabindex="-1">模型选择建议 <a class="header-anchor" href="#模型选择建议" aria-label="Permalink to &quot;模型选择建议&quot;">​</a></h2><h3 id="按使用场景选择" tabindex="-1">按使用场景选择 <a class="header-anchor" href="#按使用场景选择" aria-label="Permalink to &quot;按使用场景选择&quot;">​</a></h3><table tabindex="0"><thead><tr><th>使用场景</th><th>推荐模型</th><th>理由</th></tr></thead><tbody><tr><td><strong>日常编程</strong></td><td>Qwen-Turbo</td><td>响应快速，成本低</td></tr><tr><td><strong>复杂任务</strong></td><td>DeepSeek-R1 (thinking)</td><td>推理能力强，分析深入</td></tr><tr><td><strong>本地部署</strong></td><td>Ollama</td><td>数据隐私，离线可用</td></tr><tr><td><strong>企业级应用</strong></td><td>Amazon Bedrock</td><td>稳定可靠，安全合规</td></tr><tr><td><strong>多语言开发</strong></td><td>Qwen-Plus (thinking)</td><td>支持多语言，理解能力强</td></tr><tr><td><strong>快速原型</strong></td><td>GLM-4.6</td><td>生成速度快，适合迭代</td></tr></tbody></table><h3 id="按性能需求选择" tabindex="-1">按性能需求选择 <a class="header-anchor" href="#按性能需求选择" aria-label="Permalink to &quot;按性能需求选择&quot;">​</a></h3><h4 id="追求速度" tabindex="-1">追求速度 <a class="header-anchor" href="#追求速度" aria-label="Permalink to &quot;追求速度&quot;">​</a></h4><ul><li><strong>Qwen-Turbo</strong> - 最快响应</li><li><strong>GLM-4.6</strong> - 平衡性能与质量</li></ul><h4 id="追求质量" tabindex="-1">追求质量 <a class="header-anchor" href="#追求质量" aria-label="Permalink to &quot;追求质量&quot;">​</a></h4><ul><li><strong>DeepSeek-R1 (thinking)</strong> - 最强推理</li><li><strong>DeepSeek-V3.2</strong> - 复杂分析</li></ul><h4 id="追求成本效益" tabindex="-1">追求成本效益 <a class="header-anchor" href="#追求成本效益" aria-label="Permalink to &quot;追求成本效益&quot;">​</a></h4><ul><li><strong>Qwen-Turbo</strong> - 成本最低</li><li><strong>Ollama 本地</strong> - 无使用费用</li></ul><h4 id="追求隐私" tabindex="-1">追求隐私 <a class="header-anchor" href="#追求隐私" aria-label="Permalink to &quot;追求隐私&quot;">​</a></h4><ul><li><strong>Ollama 本地</strong> - 完全本地化</li><li><strong>Amazon Bedrock</strong> - 企业级安全</li></ul>',52),l=[n];function h(s,i,g,b,c,u){return r(),d("div",null,l)}const k=t(o,[["render",h]]);export{m as __pageData,k as default};
